{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hack4Health: Heart Disease Risk Prediction (Tabular)\n",
    "\n",
    "This notebook trains and evaluates machine learning models to predict **HeartDisease** from routine clinical + ECG-derived features.\n",
    "\n",
    "**Dataset:** `b2b/Datasets/Heart Attack/heart_processed.csv`\n",
    "\n",
    "## Reproducibility\n",
    "- Fixed random seed\n",
    "- Stratified train/test split\n",
    "- Cross-validation for model selection\n",
    "\n",
    "> If running in Google Colab: upload the `b2b` folder or adjust `DATA_PATH`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# If a required package is missing, install dependencies into the current kernel.\n",
    "# (Works in Jupyter/Colab; may take a minute on first run.)\n",
    "try:\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "except ModuleNotFoundError:\n",
    "    !{sys.executable} -m pip -q install numpy pandas scikit-learn matplotlib seaborn\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "\n",
    "try:\n",
    "    import xgboost\n",
    "    from xgboost import XGBClassifier\n",
    "except ModuleNotFoundError:\n",
    "    !{sys.executable} -m pip -q install xgboost\n",
    "    import xgboost\n",
    "    from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_validate\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, average_precision_score, accuracy_score, f1_score,\n",
    "    confusion_matrix, classification_report, RocCurveDisplay, PrecisionRecallDisplay\n",
    ")\n",
    "from sklearn.calibration import calibration_curve\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "sns.set_theme(style='whitegrid')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Robust path resolution for VS Code / Jupyter.\n",
    "# This notebook lives in the 'b2b' folder, but the working directory can vary.\n",
    "\n",
    "cwd = Path.cwd()\n",
    "\n",
    "# Candidate locations (try relative to current working directory, and relative to this notebook's folder name if present)\n",
    "candidates = [\n",
    "    cwd / 'b2b' / 'Datasets' / 'Heart Attack' / 'heart_processed.csv',\n",
    "    cwd / 'Datasets' / 'Heart Attack' / 'heart_processed.csv',\n",
    "]\n",
    "\n",
    "# Also try walking up parent directories and looking for a 'b2b' folder\n",
    "for p in [cwd] + list(cwd.parents):\n",
    "    candidates.append(p / 'b2b' / 'Datasets' / 'Heart Attack' / 'heart_processed.csv')\n",
    "\n",
    "DATA_PATH = None\n",
    "for c in candidates:\n",
    "    if c.exists():\n",
    "        DATA_PATH = c\n",
    "        break\n",
    "\n",
    "if DATA_PATH is None:\n",
    "    raise FileNotFoundError(\n",
    "        \"Could not find heart_processed.csv. Tried:\\n\" + \"\\n\".join(str(x) for x in candidates)\n",
    "    )\n",
    "\n",
    "print('Using dataset at:', DATA_PATH)\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape, df.columns.tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Data checks\n",
    "We verify:\n",
    "- Target exists and is binary\n",
    "- No missing values\n",
    "- Boolean columns are valid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert 'HeartDisease' in df.columns\n",
    "df['HeartDisease'].value_counts(dropna=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing = df.isna().mean().sort_values(ascending=False)\n",
    "missing.head(15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert boolean columns (True/False) to 0/1 (safe if already numeric)\n",
    "for col in df.columns:\n",
    "    if df[col].dtype == 'bool':\n",
    "        df[col] = df[col].astype(int)\n",
    "\n",
    "# Also handle string 'True'/'False' just in case\n",
    "for col in df.columns:\n",
    "    if df[col].dtype == 'object':\n",
    "        unique = set(df[col].dropna().unique().tolist())\n",
    "        if unique.issubset({'True', 'False'}):\n",
    "            df[col] = df[col].map({'False': 0, 'True': 1}).astype(int)\n",
    "\n",
    "df.dtypes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Quick EDA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_rate = df['HeartDisease'].mean()\n",
    "print(f'Positive class prevalence (HeartDisease=1): {target_rate:.3f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = [c for c in df.columns if c != 'HeartDisease']\n",
    "df[num_cols].describe().T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,4))\n",
    "sns.countplot(data=df, x='HeartDisease')\n",
    "plt.title('Target distribution')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation with target (quick sanity check; not causal)\n",
    "corr = df.corr(numeric_only=True)['HeartDisease'].sort_values(ascending=False)\n",
    "corr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Train/test split\n",
    "We keep a hold-out test set for a final unbiased estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=['HeartDisease'])\n",
    "y = df['HeartDisease'].astype(int)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y\n",
    ")\n",
    "X_train.shape, X_test.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Models\n",
    "We compare:\n",
    "- **Logistic Regression (scaled)**: strong baseline + interpretable\n",
    "- **Random Forest**: nonlinear model, handles interactions\n",
    "\n",
    "Evaluation metrics:\n",
    "- ROC-AUC\n",
    "- PR-AUC (Average Precision)\n",
    "- F1, Accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('model', LogisticRegression(max_iter=500, class_weight='balanced', random_state=RANDOM_STATE))\n",
    "])\n",
    "\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=600,\n",
    "    random_state=RANDOM_STATE,\n",
    "    class_weight='balanced',\n",
    "    min_samples_leaf=2,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "xgb = XGBClassifier(\n",
    "    n_estimators=600,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=4,\n",
    "    subsample=0.9,\n",
    "    colsample_bytree=0.9,\n",
    "    reg_lambda=1.0,\n",
    "    min_child_weight=1,\n",
    "    gamma=0,\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=-1,\n",
    "    eval_metric='logloss'\n",
    ")\n",
    "\n",
    "models = {\n",
    "    'LogReg (scaled)': logreg,\n",
    "    'RandomForest': rf,\n",
    "    'XGBoost': xgb,\n",
    "}\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "scoring = {\n",
    "    'roc_auc': 'roc_auc',\n",
    "    'avg_precision': 'average_precision',\n",
    "    'accuracy': 'accuracy',\n",
    "    'f1': 'f1'\n",
    "}\n",
    "\n",
    "rows = []\n",
    "for name, m in models.items():\n",
    "    scores = cross_validate(m, X_train, y_train, cv=cv, scoring=scoring, n_jobs=-1)\n",
    "    rows.append({\n",
    "        'model': name,\n",
    "        'cv_roc_auc_mean': np.mean(scores['test_roc_auc']),\n",
    "        'cv_pr_auc_mean': np.mean(scores['test_avg_precision']),\n",
    "        'cv_f1_mean': np.mean(scores['test_f1']),\n",
    "        'cv_accuracy_mean': np.mean(scores['test_accuracy']),\n",
    "    })\n",
    "\n",
    "pd.DataFrame(rows).sort_values('cv_roc_auc_mean', ascending=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Final evaluation on test set\n",
    "We fit models on the full training set, then evaluate once on the holdout test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, X_train, y_train, X_test, y_test, name):\n",
    "    model.fit(X_train, y_train)\n",
    "    proba = model.predict_proba(X_test)[:, 1]\n",
    "    pred = (proba >= 0.5).astype(int)\n",
    "\n",
    "    out = {\n",
    "        'model': name,\n",
    "        'roc_auc': roc_auc_score(y_test, proba),\n",
    "        'pr_auc': average_precision_score(y_test, proba),\n",
    "        'accuracy': accuracy_score(y_test, pred),\n",
    "        'f1': f1_score(y_test, pred),\n",
    "    }\n",
    "\n",
    "    print(name)\n",
    "    print(out)\n",
    "    print('Confusion matrix:\\n', confusion_matrix(y_test, pred))\n",
    "    print('\\nClassification report:\\n', classification_report(y_test, pred, digits=3))\n",
    "\n",
    "    return out, proba\n",
    "\n",
    "\n",
    "test_rows = []\n",
    "test_probas = {}\n",
    "for name, m in models.items():\n",
    "    out, proba = eval_model(m, X_train, y_train, X_test, y_test, name)\n",
    "    test_rows.append(out)\n",
    "    test_probas[name] = proba\n",
    "\n",
    "pd.DataFrame(test_rows).sort_values('roc_auc', ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,5))\n",
    "for name, proba in test_probas.items():\n",
    "    RocCurveDisplay.from_predictions(y_test, proba, name=name)\n",
    "plt.title('ROC curves (test set)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,5))\n",
    "for name, proba in test_probas.items():\n",
    "    PrecisionRecallDisplay.from_predictions(y_test, proba, name=name)\n",
    "plt.title('Precision-Recall curves (test set)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Calibration\n",
    "Calibration matters for risk prediction: we want probabilities that reflect true risk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,5))\n",
    "for name, proba in test_probas.items():\n",
    "    frac_pos, mean_pred = calibration_curve(y_test, proba, n_bins=10, strategy='quantile')\n",
    "    plt.plot(mean_pred, frac_pos, marker='o', label=name)\n",
    "\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', color='black')\n",
    "plt.xlabel('Mean predicted probability')\n",
    "plt.ylabel('Fraction of positives')\n",
    "plt.title('Calibration plot (test set)')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7) Interpretability (Permutation importance)\n",
    "Permutation importance measures how much model performance drops when a feature is shuffled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_name = max(test_rows, key=lambda r: r['roc_auc'])['model']\n",
    "best_model = models[best_name]\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "perm = permutation_importance(\n",
    "    best_model, X_test, y_test,\n",
    "    n_repeats=20, random_state=RANDOM_STATE, scoring='roc_auc'\n",
    ")\n",
    "\n",
    "imp = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'importance_mean': perm.importances_mean,\n",
    "    'importance_std': perm.importances_std\n",
    "}).sort_values('importance_mean', ascending=False)\n",
    "\n",
    "imp.head(15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topk = 12\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.barplot(data=imp.head(topk), x='importance_mean', y='feature', orient='h')\n",
    "plt.title(f'Permutation importance (ROC-AUC drop) - {best_name} (top {topk})')\n",
    "plt.xlabel('Mean importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8) What to write in the report\n",
    "Include:\n",
    "- Problem framing (why early detection matters)\n",
    "- Data description + preprocessing\n",
    "- Model comparison table (CV + test)\n",
    "- 1 ROC curve + 1 calibration plot\n",
    "- Top features + interpretation + limitations\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
